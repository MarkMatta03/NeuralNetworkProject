# Neural Network From Scratch â€” Part 1 & Part 2

This project implements a complete neural network **from scratch using NumPy only**, without using deep learning frameworks such as TensorFlow or PyTorch for the core implementation.  
This work fulfills **Part 1 and Part 2** of the semester project requirements.

---

## ğŸ¯ Objectives of Part 1

âœ” Build a modular neural network library  
âœ” Implement:
- Dense (Fully Connected) Layers  
- Activation Functions (Sigmoid, Tanh)  
- Mean Squared Error (MSE) Loss  
- Stochastic Gradient Descent (SGD) optimizer  

âœ” Train the model to learn the XOR logic function  
âœ” Perform Gradient Checking to verify correctness of backpropagation  
âœ” Present training results in a Jupyter Notebook  

---

## ğŸ§  XOR Problem Training

The XOR truth table:

| Input | Output |
|------|--------|
| (0,0) | 0 |
| (0,1) | 1 |
| (1,0) | 1 |
| (1,1) | 0 |

The neural network architecture used:

Input(2) â†’ Dense(4) + Tanh â†’ Dense(1) + Sigmoid  

Training Configuration:

- Loss function: **MSE**
- Optimization: **SGD**
- Epochs: **50,000**

### âœ” Final XOR Predictions

[[0.01]
[0.98]
[0.98]
[0.02]]

â¡ The model successfully learns XOR ğŸ‰

---

## ğŸ“ˆ Training Loss Curve

The loss smoothly approaches ~0 during training.

ğŸ“ Included inside:  
notebooks/project_demo.ipynb

---

## ğŸ§ª Gradient Checking

To ensure the correctness of backpropagation:

- Numerical gradients were calculated using finite differences  
- Compared with analytical gradients from the backward pass  

Result:  
Maximum difference â‰ˆ **1e-5**  
âœ” Confirms backpropagation implementation is correct  

---

## ğŸ¯ Objectives of Part 2

âœ” Apply the custom neural network library to a real dataset  
âœ” Train an autoencoder for unsupervised learning  
âœ” Reconstruct input images  
âœ” Extract latent features  
âœ” Perform classification using latent representations  
âœ” Compare results with a TensorFlow/Keras reference model  

---

## ğŸ–¼ï¸ Autoencoder on MNIST Dataset

Dataset used:
- MNIST handwritten digits  
- Input size: 784  
- Pixel values normalized to range [0, 1]  

Autoencoder architecture:

Encoder: 784 â†’ 256 â†’ 64  
Decoder:  64 â†’ 256 â†’ 784  

Training Configuration:

- Loss function: **MSE**
- Optimization: **SGD**

â¡ The autoencoder successfully reconstructs digit images.

---

## ğŸ¯ Latent Space Classification

- Latent features extracted from the encoder (64 dimensions)  
- Support Vector Machine (SVM) trained using:
  - Raw pixels (baseline)
  - Latent features (autoencoder output)

âœ” Latent features achieve comparable or better accuracy with much lower dimensionality.

---

## ğŸ” TensorFlow / Keras Comparison

A reference autoencoder was implemented using **TensorFlow/Keras** with the same architecture and loss function.

â¡ Used only for validation and comparison with the custom implementation.

---

## ğŸ“ Project Structure

NeuralNetworkProject/
â”‚
â”œâ”€ lib/
â”‚ â”œâ”€ layers.py # Dense layers
â”‚ â”œâ”€ activations.py # Sigmoid & Tanh
â”‚ â”œâ”€ losses.py # MSE + gradient
â”‚ â”œâ”€ optimizer.py # SGD optimizer
â”‚ â””â”€ network.py # Sequential model container
â”‚
â”œâ”€ notebooks/
â”‚ â””â”€ project_demo.ipynb # Part 1 & Part 2 report & results
â”‚
â”œâ”€ xor_mse_test.py # XOR test script
â”œâ”€ requirements.txt
â””â”€ README.md

---

## â–¶ï¸ How to Run

Open terminal in project root:

```bash
python xor_mse_test.py
Or open the notebook:
notebooks/project_demo.ipynb
Run all cells from top to bottom using the Python 3.11 kernel.
