

```markdown
# Neural Network From Scratch â€” Part 1 & Part 2

This project implements a complete neural network **from scratch using NumPy**, without relying on deep learning frameworks for the core implementation.  
The work is divided into **Part 1** (fundamentals and XOR) and **Part 2** (autoencoder on MNIST with latent-space classification).

---

## ğŸ¯ Part 1 â€” Neural Network Fundamentals

### Objectives

âœ” Build a modular neural network library  
âœ” Implement:
- Dense (Fully Connected) layers  
- Activation functions (Sigmoid, Tanh)  
- Mean Squared Error (MSE) loss  
- Stochastic Gradient Descent (SGD) optimizer  

âœ” Train a neural network to learn the XOR logic function  
âœ” Perform gradient checking to verify backpropagation correctness  

---

### ğŸ§  XOR Problem

**XOR Truth Table**

| Input | Output |
|------|--------|
| (0, 0) | 0 |
| (0, 1) | 1 |
| (1, 0) | 1 |
| (1, 1) | 0 |

**Network Architecture**
```

Input (2)
â†’ Dense (4) + Tanh
â†’ Dense (1) + Sigmoid

```

**Training Configuration**
- Loss: Mean Squared Error (MSE)
- Optimizer: SGD
- Epochs: 50,000

**Final Predictions**
```

[[0.01]
[0.98]
[0.98]
[0.02]]

```

âœ” The network successfully learns the XOR function.

---

### ğŸ§ª Gradient Checking

- Numerical gradients computed using finite differences  
- Compared with analytical gradients from backpropagation  

**Result:**  
Maximum difference â‰ˆ **1e-5**, confirming correctness of the implementation.

---

## ğŸ¯ Part 2 â€” Autoencoder on MNIST

### Objectives

âœ” Apply the custom neural network library to a real dataset  
âœ” Train an autoencoder for unsupervised representation learning  
âœ” Visualize image reconstruction quality  
âœ” Use latent features for classification  
âœ” Validate results using a TensorFlow/Keras reference model  

---

### ğŸ–¼ï¸ MNIST Autoencoder

**Dataset**
- MNIST handwritten digits
- Input dimension: 784
- Normalized to range [0, 1]

**Architecture**
```

Encoder: 784 â†’ 256 â†’ 64
Decoder:  64 â†’ 256 â†’ 784

```

- Activations: Tanh (hidden), Sigmoid (output)
- Loss: Mean Squared Error (MSE)
- Optimizer: SGD

âœ” The autoencoder successfully reconstructs digit images.

---

### ğŸ¯ Latent Space Classification

- Latent vectors (64-D) extracted from the encoder  
- Support Vector Machine (SVM) trained on:
  - Raw pixels (baseline)
  - Latent features (autoencoder output)

âœ” Latent features achieve comparable or better accuracy with much lower dimensionality.

---

### ğŸ” TensorFlow / Keras Comparison

A reference autoencoder is implemented using **TensorFlow/Keras** with the same architecture and loss function to validate the correctness of the custom implementation.

> TensorFlow is used **only for comparison**, not for the main implementation.

---

## ğŸ“ Project Structure

```

NeuralNetworkProject/
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ layers.py        # Dense layers
â”‚   â”œâ”€â”€ activations.py  # Sigmoid & Tanh
â”‚   â”œâ”€â”€ losses.py       # MSE loss
â”‚   â”œâ”€â”€ optimizer.py    # SGD optimizer
â”‚   â””â”€â”€ network.py      # Sequential container
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ project_demo.ipynb   # Part 1 & Part 2 results
â”‚
â”œâ”€â”€ xor_mse_test.py     # XOR test script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## â–¶ï¸ How to Run

**XOR Test**
```bash
python xor_mse_test.py
````

**Full Project Demo**
Open:

```
notebooks/project_demo.ipynb
```

Run all cells using the **Python 3.11** kernel.

---

## ğŸ› ï¸ Environment & Dependencies

* Python **3.11**
* Required libraries:

```
numpy
matplotlib
scikit-learn
pandas
tensorflow
```

Install with:

```bash
pip install -r requirements.txt
```

